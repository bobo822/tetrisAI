# DDQN+GA Tetris AI 專案實現詳細說明

## 專案概述

本專案實現了一個基於雙重深度Q網絡（DDQN）和遺傳算法（GA）的俄羅斯方塊AI系統，能夠通過TCP通信與Java Tetris伺服器交互，並通過智能學習策略獲得高分。

## 核心技術架構

### 1. Dueling Deep Q-Network (DDQN)

**架構特點：**
- 將Q網絡分為兩個流：價值流（Value Stream）和優勢流（Advantage Stream）
- 價值流評估狀態V(s)，優勢流評估每個動作的相對優勢A(s,a)
- 最終Q值通過公式合併：Q(s,a) = V(s) + A(s,a) - mean(A(s,a))

**網絡結構：**
```
輸入層(24) → 隱藏層1(128) → 隱藏層2(128)
                                    ↓
                          ┌─價值流(64→1)
                          └─優勢流(64→5)
                                    ↓
                            聚合層 → 輸出(5)
```

### 2. 遺傳算法超參數優化

**優化參數：**
- 學習率 (learning_rate)
- 折扣因子 (gamma)
- 探索率及衰減 (epsilon, epsilon_decay)
- 批次大小 (batch_size)
- 更新頻率 (update_frequency)
- 隱藏層單元數 (hidden_units)
- 經驗回放緩衝區大小 (replay_buffer_size)

**GA流程：**
1. 初始化種群：生成隨機超參數組合
2. 適應度評估：每個個體訓練DDQN並測試性能
3. 選擇：基於輪盤賭方法選擇優秀個體
4. 交叉：結合父母個體的超參數
5. 變異：隨機修改部分超參數
6. 精英策略：保留最優個體到下一代

### 3. TCP通信協議

**通信格式：**
- 客戶端→伺服器：文本命令（start, move, rotate, drop）
- 伺服器→客戶端：二進制響應（遊戲狀態 + PNG圖像）

**響應格式：**
```
1 byte: 遊戲結束標誌
4 bytes: 消除行數
4 bytes: 堆疊高度
4 bytes: 洞穴數量
4 bytes: 圖像大小
variable: PNG圖像數據
```

## 實現細節

### 4. 環境設計

**狀態空間（24維）：**
- 基本特徵：高度、洞數、消除行數、步數（4維）
- 圖像特徵：20個關鍵像素點的灰度值（20維）

**動作空間（5種）：**
0. 向左移動
1. 向右移動
2. 逆時針旋轉
3. 順時針旋轉
4. 下落

**獎勵函數：**
```python
reward = lines_cleared * 1000      # 消除行數獎勵
       - height_change * 5         # 高度增加懲罰
       - holes_change * 10         # 洞數增加懲罰
       + drop_bonus * 5            # 下落動作小獎勵
       + survival_bonus * 1        # 生存獎勵
```

### 5. 訓練策略

**經驗回放：**
- 存儲 (state, action, reward, next_state, done) 經驗元組
- 隨機採樣減少相關性
- 緩衝區大小可通過GA優化

**ε-貪婪策略：**
- 初始探索率：1.0
- 最小探索率：0.01
- 衰減率：0.995（可通過GA優化）

**目標網絡更新：**
- 每1000步更新一次目標網絡
- 提高訓練穩定性

### 6. 評估與記錄

**性能指標：**
- 消除行數（主要指標）
- 遊戲步數
- 獎勵總和
- 自定義評分：removed_lines + played_steps/1000000

**輸出格式：**
```csv
id,removed_lines,played_steps
0,28,15000
1,28,15000
```

**視頻錄製：**
- 使用OpenCV錄製遊戲過程
- 在視頻上疊加遊戲信息
- 支持MP4格式輸出

## 使用方法

### 訓練流程

1. **啟動Java伺服器：**
   ```bash
   java -jar TetrisTCPserver_v0.6.jar
   ```

2. **遺傳算法優化：**
   ```bash
   python main.py --mode ga
   ```

3. **使用最佳超參數訓練：**
   ```bash
   python main.py --mode train
   ```

4. **評估性能：**
   ```bash
   python main.py --mode evaluate
   ```

### 配置文件

`config.json` 包含所有可調參數：
- DDQN網絡配置
- 遺傳算法參數
- 訓練設置
- 評估選項

## 預期結果

**訓練表現：**
- 初期：隨機策略，低分
- 中期：學會基本消除，10-15行
- 後期：掌握高級策略，20-30行

**GA優化效果：**
- 自動找到最佳學習率和網絡結構
- 比手動調參提升10-20%性能
- 減少調參時間和人工介入

**最終目標：**
- 穩定消除25行以上
- 遊戲時長超過10000步
- 自定義評分>25.01

## 技術亮點

1. **創新結合**：首次將DDQN與GA結合用於Tetris
2. **自動化**：全自動超參數優化流程
3. **實用性**：真實TCP通信，可部署應用
4. **可視化**：完整的訓練監控和結果記錄
5. **可擴展**：模組化設計，易於修改和擴展